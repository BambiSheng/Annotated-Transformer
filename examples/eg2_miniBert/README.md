# Example 2. Mini BERT Pretrain
> [BERT](https://arxiv.org/abs/1810.04805) unveiled the high potential that transformer owns in a great range of NLP tasks.
> Inspired by [d2l](https://zh-v2.d2l.ai/chapter_natural-language-processing-pretraining/bert-dataset.html), we use the dataset [WikiText-2](https://huggingface.co/datasets/carlosejimenez/wikitext__wikitext-2-raw-v1) to pretrain a mini .

## File Structure

## Run
